{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNoYeYub1auYh6RxQIh7mC1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gv6my4KDr0j"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove downloads\n",
        "!rm -rf /content/YOLO\n",
        "!rm -rf /content/COCO\n",
        "!rm -rf /content/YOLO.zip\n",
        "!rm -rf /content/COCO.zip\n",
        "!rm -rf /content/sample_data"
      ],
      "metadata": {
        "id": "jTEG5sFgRHT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download YOLO and COCO zip files from shared Google Drive via their links.\n",
        "\n",
        "**YOLO dataset:** Split into *train* and *valid* folders each containing subfolders *images* and *labels*. All hotdog images from *images* are .jpg files and associated with one .txt file from *labels*.\n",
        "\n",
        "**COCO dataset:** Split into *train* and *valid* folders. Each folder is populated with .jpg files of hotdog images and a single .json file containing annotations for all images within the folder."
      ],
      "metadata": {
        "id": "N3UOmhJ3aUjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "\n",
        "# YOLO drive link\n",
        "yolo_id = \"1_w6_UtCZfdx2JYbrzVNyUn10dzdKzV4y\"\n",
        "!gdown --id {yolo_id} -O YOLO.zip\n",
        "# unzip YOLO\n",
        "!unzip -q YOLO.zip -d /content/YOLO\n",
        "\n",
        "# COCO drive link\n",
        "coco_id = \"1dTfSNQ9Qk_T4BOuS1l0cdDQS5xtXrVJ4\"\n",
        "!gdown --id {coco_id} -O COCO.zip\n",
        "# unzip COCO\n",
        "!unzip -q COCO.zip -d /content/COCO\n",
        "\n",
        "# check downloads\n",
        "print('\\n\\n' + '-'*20)\n",
        "print('Checking YOLO downloads, expecting \"train\" and \"valid\" folders:')\n",
        "!ls \"/content/YOLO/Hot Dog Detection YOLO\"\n",
        "print('Checking COCO downloads, expecting \"train\" and \"valid\" folders:')\n",
        "!ls \"/content/COCO/Hot Dog Detection COCO\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHtwAtUfK3Iv",
        "outputId": "409859ea-79e3-4d84-9fb4-3c6ed310f124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_w6_UtCZfdx2JYbrzVNyUn10dzdKzV4y\n",
            "From (redirected): https://drive.google.com/uc?id=1_w6_UtCZfdx2JYbrzVNyUn10dzdKzV4y&confirm=t&uuid=aec25eb4-51e9-4d1f-bce6-441dec41b583\n",
            "To: /content/YOLO.zip\n",
            "100% 44.2M/44.2M [00:00<00:00, 75.9MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1dTfSNQ9Qk_T4BOuS1l0cdDQS5xtXrVJ4\n",
            "From (redirected): https://drive.google.com/uc?id=1dTfSNQ9Qk_T4BOuS1l0cdDQS5xtXrVJ4&confirm=t&uuid=10935cc4-67f5-452d-91e0-a69eb67b0b6e\n",
            "To: /content/COCO.zip\n",
            "100% 43.4M/43.4M [00:00<00:00, 49.0MB/s]\n",
            "\n",
            "\n",
            "--------------------\n",
            "Checking YOLO downloads, expecting \"train\" and \"valid\" folders:\n",
            "train  valid\n",
            "Checking COCO downloads, expecting \"train\" and \"valid\" folders:\n",
            "train  valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze JSON structure used in COCO annotations. To synthezise COCO with the YOLO dataset (and for our hotdog detection use-case) we need both bounding box labels and object class. Note: These labels will need to be further processed to match YOLO's format."
      ],
      "metadata": {
        "id": "4nOzbpjHsCId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# see COCO json structure\n",
        "coco_json = Path(r\"/content/COCO/Hot Dog Detection COCO/train/_annotations.coco.json\")\n",
        "with open(coco_json, 'r') as f:\n",
        "  data = json.load(f)\n",
        "# print(json.dumps(data, indent=4))\n",
        "\n",
        "def print_json_structure(d, indent=0):\n",
        "    if isinstance(d, dict):\n",
        "        for key, value in d.items():\n",
        "            print(\"  \" * indent + str(key))\n",
        "            print_json_structure(value, indent + 2)\n",
        "    elif isinstance(d, list):\n",
        "        print(\"  \" * indent + \"[list]\")\n",
        "        if len(d) > 0:\n",
        "            print_json_structure(d[0], indent + 2)\n",
        "\n",
        "print_json_structure(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86FZ9QfMTtR9",
        "outputId": "f5e70b44-044e-4a3c-99bd-3f18570b6f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images\n",
            "    [list]\n",
            "        id\n",
            "        license\n",
            "        file_name\n",
            "        height\n",
            "        width\n",
            "        date_captured\n",
            "annotations\n",
            "    [list]\n",
            "        id\n",
            "        image_id\n",
            "        category_id\n",
            "        bbox\n",
            "            [list]\n",
            "        area\n",
            "        segmentation\n",
            "            [list]\n",
            "        iscrowd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create **custom PyTorch Dataset** to both load and synthesize both YOLO and COCO datasets. This can be done by enforcing the same image and label formats between datasets, which I will choose to convert COCO data to YOLO formatting.\n",
        "\n",
        "Note: For memory purposes, both images and labels are loaded and stored as file paths to avoid loading thousands of ndarrays into RAM at once. Instead the actual values will be loaded in per batch. This makes the dataset lighweight and resulting scalable."
      ],
      "metadata": {
        "id": "RDBiRiaGtKyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HotDogDataset(Dataset):\n",
        "    def __init__(self, yolo_src_path, coco_src_path, subset='train', transform=None):\n",
        "        '''\n",
        "        Args:\n",
        "        yolo_src_path (Path): Path to source YOLO data folder.\n",
        "        coco_src_path (Path): Path to source COCO data folder.\n",
        "        subset (str): Set to be loaded. Either 'train' or 'valid'.\n",
        "        '''\n",
        "        self.yolo_root = yolo_src_path / subset\n",
        "        self.coco_root = coco_src_path / subset\n",
        "        self.transform = transform\n",
        "        # only store file names in memory, process them on the fly <- __getitem__\n",
        "        self.image_files = []\n",
        "        self.label_files = []\n",
        "        self.fetch_data_paths()\n",
        "\n",
        "\n",
        "    def fetch_data_paths(self):\n",
        "        # fetch YOLO files\n",
        "        img_dir = self.yolo_root / 'images'\n",
        "        label_dir = self.yolo_root / 'labels'\n",
        "        for img_path in list(img_dir.glob('*.jpg')):\n",
        "            associated_label = label_dir / (img_path.stem + '.txt')\n",
        "            if associated_label.exists():\n",
        "                self.image_files.append(img_path)\n",
        "                self.label_files.append(associated_label)\n",
        "\n",
        "        # fetch COCO files\n",
        "        annotations = self.coco_root / '_annotations.coco.json'\n",
        "        with open(annotations, 'r') as f:\n",
        "            coco_data = json.load(f)\n",
        "\n",
        "        # goal is to get id from image then map id onto annotations key to get associated labels (eg. bbox)\n",
        "        # first, create map + grouping annotations from the same image\n",
        "        self.id_to_annotation = {}\n",
        "        for annotation in coco_data['annotations']:\n",
        "            self.id_to_annotation.setdefault(annotation['image_id'], []).append(annotation)\n",
        "        # match id to filename <- filename is value in image dict\n",
        "        self.img_metadata = {img['id']: img for img in coco_data['images']}\n",
        "\n",
        "        for img_id, metadata in self.img_metadata.items():\n",
        "            img_path = self.coco_root / metadata['file_name']\n",
        "            if img_path.exists():\n",
        "                self.image_files.append(img_path)\n",
        "                self.label_files.append(img_id) # later used to map onto annotations using self.id_to_annotation dict\n",
        "\n",
        "\n",
        "    def _yolo_label(self, label_file):\n",
        "        '''Takes yolo formatted label file (.txt) and extracts usable label.'''\n",
        "        # YOLO labels are in format \"class_id x_center y_center box_width box_height\"\n",
        "        # the values are normalized between 0 and 1, propotional to image size.\n",
        "        with open(label_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            labels = np.array([line.strip().split() for line in lines], dtype=np.float32)\n",
        "\n",
        "        return labels[:,1:], labels[:,0].astype(np.int32) # all bboxes, all classes\n",
        "\n",
        "\n",
        "    def _coco_label(self, img_id):\n",
        "        '''Takes id of coco image and fetches the associated bbox and class label from\n",
        "        annotation dict. Converting to YOLO format.'''\n",
        "        bboxes, classes = [], []\n",
        "        img_width, img_height = self.img_metadata[img_id]['width'], self.img_metadata[img_id]['height'] # for normalization\n",
        "\n",
        "        for annotation in self.id_to_annotation.get(img_id, []):\n",
        "            x_top_left, y_top_left, w, h = annotation['bbox']\n",
        "            # center x and y and normalize all, 0 to 1\n",
        "            norm_cx = (x_top_left + w/2) / img_width\n",
        "            norm_cy = (y_top_left + h/2) / img_height\n",
        "            norm_w = w / img_width\n",
        "            norm_h = h / img_height\n",
        "\n",
        "            bboxes.append([norm_cx, norm_cy, norm_w, norm_h])\n",
        "            classes.append(annotation['category_id'])\n",
        "\n",
        "        return np.array(bboxes, dtype=np.float32), np.array(classes, dtype=np.int32)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        img = cv2.imread(str(img_path)) # cv2 loads image in BGR\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # convert to RGB\n",
        "\n",
        "        labels = self.label_files[idx]\n",
        "\n",
        "        # YOLO case, label (Path): path to .txt file\n",
        "        if isinstance(labels, Path):\n",
        "            bboxes, classes = self._yolo_label(labels)\n",
        "        # COCO case, label (int): image_id\n",
        "        elif isinstance(labels, int):\n",
        "            bboxes, classes = self._coco_label(labels)\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(img, bboxes, classes)\n",
        "            img, bboxes, classes = transformed['image'], transformed['bboxes'], transformed['classes']\n",
        "        else:\n",
        "            img = cv2.resize(img, (224,224))\n",
        "            img = transforms.ToTensor()(img) # handles 0 to 1 normalization and permute -> (C,H,W)\n",
        "            # using ImageNet's normalization weights since dealing with natural images\n",
        "            img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
        "\n",
        "        bboxes_tensor = torch.tensor(bboxes, dtype=torch.float32)\n",
        "        classes_tensor = torch.tensor(classes, dtype=torch.long)\n",
        "\n",
        "        return {'image': img,\n",
        "                'targets': {'bboxes': bboxes_tensor, 'classes': classes_tensor},\n",
        "                'image_path': img_path\n",
        "        }\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "\n",
        "    # needed since pytorch cannot 'auto-batch' labels with varying number of bounding boxes, labels have\n",
        "    # different tensor sizes -> so instead of torch.stack that pytorch would do automatically, we leave labels as a list of tensors\n",
        "    # images must be stacked though\n",
        "    def custom_collate_fn(self, batch):\n",
        "        batch_images, batch_targets, batch_image_paths = [], [], []\n",
        "        for sample in batch:\n",
        "            batch_images.append(sample['image'])\n",
        "            batch_targets.append(sample['targets'])\n",
        "            batch_image_paths.append(sample['image_path'])\n",
        "\n",
        "        batch_images = torch.stack(batch_images)\n",
        "        return {'batch_images': batch_images,\n",
        "                'batch_targets': batch_targets,\n",
        "                'batch_image_paths': batch_image_paths\n",
        "        }"
      ],
      "metadata": {
        "id": "3X7rJhspaqmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize dataset and dataloaders** using the custom Pytorch dataset created above. Sticking to just train and validation sets right now since that's how the downloaded data was presented (can always change the split)."
      ],
      "metadata": {
        "id": "lOf47yKqYmfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_src_path = Path(\"/content/YOLO/Hot Dog Detection YOLO\")\n",
        "coco_src_path = Path(\"/content/COCO/Hot Dog Detection COCO\")\n",
        "\n",
        "# datasets\n",
        "train_dataset = HotDogDataset(yolo_src_path, coco_src_path, subset='train')\n",
        "valid_dataset = HotDogDataset(yolo_src_path, coco_src_path, subset='valid')\n",
        "\n",
        "# dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=train_dataset.custom_collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, collate_fn=valid_dataset.custom_collate_fn)"
      ],
      "metadata": {
        "id": "fnAOM-xeLtSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the batch output (shape). Batched images should be a tensor of shape (batch_size, C, H, W). Both amount of targets and image paths should be identical to the batch_size the loader was initialized with. The bboxes target should be shape(num_annotated_boxes, 4) and the classes target should be (num_annotated_boxes)."
      ],
      "metadata": {
        "id": "onrNU3RrlcBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch output <- what is returned after collate_fn\n",
        "for i in range(2):\n",
        "  batch = next(iter(train_loader))\n",
        "  print(f'Example Batch {i+1}\\n' + '-'*20)\n",
        "  print(f\"Batched images shape: {batch['batch_images'].shape}\")\n",
        "  print(f\"Amount of targets: {len(batch['batch_targets'])}\")\n",
        "  print(f\"   First bboxes annotation (in batch) shape: {batch['batch_targets'][0]['bboxes'].shape}\")\n",
        "  print(f\"   First classes annotation (in batch) shape: {batch['batch_targets'][0]['classes'].shape}\")\n",
        "  print(f\"Number of batch's image paths: {len(batch['batch_image_paths'])}\\n\") # should give batch_size <- sanity check\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xuCVXv-eDHx",
        "outputId": "d34b078e-4ed2-498e-fb8e-455c957da57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Batch 1\n",
            "--------------------\n",
            "Batched images shape: torch.Size([4, 3, 224, 224])\n",
            "Amount of targets: 4\n",
            "   First bboxes annotation (in batch) shape: torch.Size([3, 4])\n",
            "   First classes annotation (in batch) shape: torch.Size([3])\n",
            "Number of batch's image paths: 4\n",
            "\n",
            "Example Batch 2\n",
            "--------------------\n",
            "Batched images shape: torch.Size([4, 3, 224, 224])\n",
            "Amount of targets: 4\n",
            "   First bboxes annotation (in batch) shape: torch.Size([1, 4])\n",
            "   First classes annotation (in batch) shape: torch.Size([1])\n",
            "Number of batch's image paths: 4\n",
            "\n"
          ]
        }
      ]
    }
  ]
}