{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPasY706ps83EjhfcSO/IMZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmanuelko-bot/hotdog_detection/blob/main/Hot_Diggity_Dogs_Vision_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8gv6my4KDr0j"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove downloads\n",
        "!rm -rf /content/YOLO\n",
        "!rm -rf /content/COCO\n",
        "!rm -rf /content/YOLO.zip\n",
        "!rm -rf /content/COCO.zip\n",
        "!rm -rf /content/sample_data"
      ],
      "metadata": {
        "id": "jTEG5sFgRHT9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "\n",
        "# YOLO drive link\n",
        "yolo_id = \"1_w6_UtCZfdx2JYbrzVNyUn10dzdKzV4y\"\n",
        "!gdown --id {yolo_id} -O YOLO.zip\n",
        "# unzip YOLO\n",
        "!unzip -q YOLO.zip -d /content/YOLO\n",
        "\n",
        "# COCO drive link\n",
        "coco_id = \"1dTfSNQ9Qk_T4BOuS1l0cdDQS5xtXrVJ4\"\n",
        "!gdown --id {coco_id} -O COCO.zip\n",
        "# unzip COCO\n",
        "!unzip -q COCO.zip -d /content/COCO\n",
        "\n",
        "# check downloads\n",
        "print('Checking YOLO downloads, expecting \"train\" and \"valid\" folders:')\n",
        "!ls \"/content/YOLO/Hot Dog Detection YOLO\"\n",
        "print('Checking COCO downloads, expecting \"train\" and \"valid\" folders:')\n",
        "!ls \"/content/COCO/Hot Dog Detection COCO\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHtwAtUfK3Iv",
        "outputId": "8eb08e87-b580-4a94-e1ac-73c3b02f8c5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_w6_UtCZfdx2JYbrzVNyUn10dzdKzV4y\n",
            "From (redirected): https://drive.google.com/uc?id=1_w6_UtCZfdx2JYbrzVNyUn10dzdKzV4y&confirm=t&uuid=f878b4ff-da90-4b0b-ac69-d603e6fedb06\n",
            "To: /content/YOLO.zip\n",
            "100% 44.2M/44.2M [00:00<00:00, 209MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1dTfSNQ9Qk_T4BOuS1l0cdDQS5xtXrVJ4\n",
            "From (redirected): https://drive.google.com/uc?id=1dTfSNQ9Qk_T4BOuS1l0cdDQS5xtXrVJ4&confirm=t&uuid=2a1d7996-9584-48a8-a548-0a1c74572acf\n",
            "To: /content/COCO.zip\n",
            "100% 43.4M/43.4M [00:00<00:00, 173MB/s]\n",
            "Checking YOLO downloads, expecting \"train\" and \"valid\" folders:\n",
            "train  valid\n",
            "Checking COCO downloads, expecting \"train\" and \"valid\" folders:\n",
            "train  valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# see COCO json structure\n",
        "coco_json = Path(r\"/content/COCO/Hot Dog Detection COCO/train/_annotations.coco.json\")\n",
        "with open(coco_json, 'r') as f:\n",
        "  data = json.load(f)\n",
        "# print(json.dumps(data, indent=4))\n",
        "\n",
        "def print_json_structure(d, indent=0):\n",
        "    if isinstance(d, dict):\n",
        "        for key, value in d.items():\n",
        "            print(\"  \" * indent + str(key))\n",
        "            print_json_structure(value, indent + 2)\n",
        "    elif isinstance(d, list):\n",
        "        print(\"  \" * indent + \"[list]\")\n",
        "        if len(d) > 0:\n",
        "            print_json_structure(d[0], indent + 2)\n",
        "\n",
        "print_json_structure(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86FZ9QfMTtR9",
        "outputId": "340580ec-624c-40c1-f776-5dfbabf32a34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images\n",
            "    [list]\n",
            "        id\n",
            "        license\n",
            "        file_name\n",
            "        height\n",
            "        width\n",
            "        date_captured\n",
            "annotations\n",
            "    [list]\n",
            "        id\n",
            "        image_id\n",
            "        category_id\n",
            "        bbox\n",
            "            [list]\n",
            "        area\n",
            "        segmentation\n",
            "            [list]\n",
            "        iscrowd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HotDogDataset(Dataset):\n",
        "    def __init__(self, yolo_src_path, coco_src_path, subset='train', transform=None):\n",
        "        '''\n",
        "        Args:\n",
        "        yolo_src_path (Path): Path to source YOLO data folder.\n",
        "        coco_src_path (Path): Path to source COCO data folder.\n",
        "        subset (str): Set to be loaded. Either 'train' or 'valid'.\n",
        "        '''\n",
        "        self.yolo_root = yolo_src_path / subset\n",
        "        self.coco_root = coco_src_path / subset\n",
        "        self.transform = transform\n",
        "        # only store file names in memory, process them on the fly <- __getitem__\n",
        "        self.image_files = []\n",
        "        self.label_files = []\n",
        "\n",
        "\n",
        "    def fetch_data_paths(self):\n",
        "        # fetch YOLO files\n",
        "        img_dir = self.yolo_root / 'images'\n",
        "        label_dir = self.yolo_root / 'labels'\n",
        "        for img_path in list(img_dir.glob('*.jpg')):\n",
        "            associated_label = label_dir / (img_path.stem + '.txt')\n",
        "            if associated_label.exists():\n",
        "                self.image_files.append(img_path)\n",
        "                self.label_files.append(associated_label)\n",
        "\n",
        "        # fetch COCO files\n",
        "        annotations = self.coco_root / '_annotations.coco.json'\n",
        "        with open(annotations, 'r') as f:\n",
        "            coco_data = json.load(f)\n",
        "\n",
        "        # goal is to get id from image then map id onto annotations key to get associated labels (eg. bbox)\n",
        "        # first, create map + grouping annotations from the same image\n",
        "        self.id_to_annotation = {}\n",
        "        for annotation in coco_data['annotations']:\n",
        "            self.id_to_annotation.setdefault(annotation['image_id'], []).append(annotation)\n",
        "        # match id to filename <- filename is value in image dict\n",
        "        self.img_metadata = {img['id']: img for img in coco_data['images']}\n",
        "\n",
        "        for img_id, metadata in self.img_metadata.items():\n",
        "            img_path = self.coco_root / metadata['file_name']\n",
        "            if img_path.exists():\n",
        "                self.image_files.append(img_path)\n",
        "                self.label_files.append(img_id) # later used to map onto annotations using self.id_to_annotation dict\n",
        "\n",
        "\n",
        "    def _yolo_label(self, label_file):\n",
        "        '''Takes yolo formatted label file (.txt) and extracts usable label.'''\n",
        "        # YOLO labels are in format \"class_id x_center y_center box_width box_height\"\n",
        "        # the values are normalized between 0 and 1, propotional to image size.\n",
        "        with open(label_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            labels = np.array([line.strip().split() for line in lines], dtype=np.float32)\n",
        "\n",
        "        return labels[:,1:], labels[:,0].astype(np.int32) # all bboxes, all classes\n",
        "\n",
        "\n",
        "    def _coco_label(self, img_id)\n",
        "        '''Takes id of coco image and fetches the associated bbox and class label from\n",
        "        annotation dict. Converting to YOLO format.'''\n",
        "        bboxes, classes = [], []\n",
        "        img_width, img_height = self.img_metadata[img_id]['width'], self.img_metadata[img_id]['height'] # for normalization\n",
        "\n",
        "        for annotation in self.id_to_annotation.get(img_id, []):\n",
        "            x_top_left, y_top_left, w, h = annotation['bbox']\n",
        "            # center x and y and normalize all, 0 to 1\n",
        "            norm_cx = (x_top_left + w/2) / img_width\n",
        "            norm_cy = (y_top_left + h/2) / img_height\n",
        "            norm_w = w / img_width\n",
        "            norm_h = h / img_height\n",
        "\n",
        "            bboxes.append([norm_cx, norm_cy, norm_w, norm_h])\n",
        "            classes.append(annotation['category_id'])\n",
        "\n",
        "        return np.array(bboxes, dtype=np.float32), np.array(classes, dtype=np.int32)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        img = cv2.imread(str(img_path)) # cv2 loads image in BGR\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # convert to RGB\n",
        "\n",
        "        labels = self.label_files[idx]\n",
        "\n",
        "        # YOLO case, label (Path): path to .txt file\n",
        "        if isinstance(labels, Path):\n",
        "            bboxes, classes = self._yolo_label(labels)\n",
        "        # COCO case, label (int): image_id\n",
        "        elif isinstance(labels, int):\n",
        "            bboxes, classes = self._coco_label(labels)\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(img, bboxes, classes)\n",
        "            img, bboxes, classes = transformed['image'], transformed['bboxes'], transformed['classes']\n",
        "        else:\n",
        "            img = cv2.resize(img, (224,224))\n",
        "            img = transforms.ToTensor()(img) # handles 0 to 1 normalization and permute -> (C,H,W)\n",
        "            # using ImageNet's normalization weights since dealing with natural images\n",
        "            img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
        "\n",
        "        bboxes_tensor = torch.tensor(bboxes, dtype=torch.float32)\n",
        "        classes_tensor = torch.tensor(classes, dtype=torch.long)\n",
        "\n",
        "        return {'image': img,\n",
        "                'targets': {'bboxes': bboxes_tensor, 'classes': classes_tensor},\n",
        "                'image_path': img_path\n",
        "        }\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "\n",
        "    # needed since pytorch cannot 'auto-batch' labels with varying number of bounding boxes, labels have\n",
        "    # different tensor sizes -> so instead of torch.stack that pytorch would do automatically, we leave labels as a list of tensors\n",
        "    # images must be stacked though\n",
        "    def custom_collate_fn(self, batch):\n",
        "        batch_images, batch_targets, batch_image_paths = [], [], []\n",
        "        for sample in batch:\n",
        "            batch_images.append(sample['image'])\n",
        "            batch_targets.append(sample['targets'])\n",
        "            batch_image_paths.append(sample['image_path'])\n",
        "\n",
        "        batch_images = torch.stack(batch_images)\n",
        "        return {'batch_images': batch_images,\n",
        "                'batch_targets': batch_targets,\n",
        "                'batch_image_paths': batch_image_paths\n",
        "        }"
      ],
      "metadata": {
        "id": "3X7rJhspaqmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(yolo_src_path, coco_src_path, subset='train', n_samples=None):\n",
        "  '''Loads images and labels from folder containing subfolders of 'images' and 'labels' (YOLO format).\n",
        "  Args:\n",
        "  yolo_src_path (Path): Path to source YOLO data folder.\n",
        "  coco_src_path (Path): Path to source COCO data folder.\n",
        "  subset (str): Set to be loaded. Either 'train' or 'valid'.\n",
        "  n_samples (int): Number of image samples to load. If None, load all images and their associated labels.\n",
        "  Return:\n",
        "  images: List of images, type ndarray.\n",
        "  labels: List of labels, type ndarray.\n",
        "  '''\n",
        "  images, labels = [], []\n",
        "\n",
        "  #----------------\n",
        "  # fetch YOLO data\n",
        "  #----------------\n",
        "  # fetch images and labels files\n",
        "  img_dir = yolo_src_path / subset / 'images'\n",
        "  label_dir = yolo_src_path / subset / 'labels'\n",
        "  img_files = list(img_dir.glob('*.jpg'))\n",
        "  label_files = list(label_dir.glob('*.txt'))\n",
        "\n",
        "  def load_images(img_files):\n",
        "    for img_file in img_files:\n",
        "      img = cv2.imread(str(img_file)) # cv2 loads image in BGR\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # convert to RGB\n",
        "      images.append(img)\n",
        "\n",
        "  load_images(img_files)\n",
        "\n",
        "  # YOLO labels are in format \"class_id x_center y_center box_width box_height\"\n",
        "  # the values are normalized between 0 and 1, propotional to image size\n",
        "  for label_file in label_files:\n",
        "    with open(str(label_file), 'r') as f:\n",
        "      # each line is a bounding box label\n",
        "      lines = f.readlines()\n",
        "      label = [line.strip().split() for line in lines]\n",
        "      label = np.array(label, dtype=np.float32)\n",
        "      labels.append(label)\n",
        "\n",
        "  # ---------------\n",
        "  # fetch COCO data\n",
        "  #----------------\n",
        "  data_dir = coco_src_path / subset\n",
        "  img_files = list(data_dir.glob('*,jpg'))\n",
        "  label_json = data_dir / subset / '_annotations.coco.json'\n",
        "\n",
        "  load_images(img_files)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # each image should be matched up with a label\n",
        "  assert len(images) == len(labels)\n",
        "\n",
        "  # SAMPLE\n",
        "  # take n random samples from dataset\n",
        "  if n_samples and n_samples < len(images):\n",
        "    sample_idx = np.random.choice(len(images), n_samples, replace=False)\n",
        "    images = [images[i] for i in sample_idx]\n",
        "    labels = [labels[i] for i in sample_idx]\n",
        "\n",
        "  return images, labels"
      ],
      "metadata": {
        "id": "aQPVaXebEP7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create custom pytorch dataset\n",
        "class YOLOdataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = torch.from_numpy(self.labels[idx]).float()\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img = cv2.resize(img, (224,224))\n",
        "            img = transforms.ToTensor()(img)\n",
        "            img = transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    # needed since pytorch cannot 'auto-batch' labels with varying number of bounding boxes, labels have\n",
        "    # different tensor sizes -> so instead of torch.stack that pytorch would do automatically, we leave labels as a list of tensors\n",
        "    # images must be stacked though\n",
        "    def custom_collate_fn(self, batch):\n",
        "        images, labels = zip(*batch) # unpack batch\n",
        "        images = torch.stack(images)\n",
        "        return images, labels\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "def get_loader(yolo_src_path, subset='train', n_samples=None, batch_size=4, shuffle=True):\n",
        "    images, labels = get_data(yolo_src_path, subset=subset, n_samples=n_samples) # get data from src\n",
        "\n",
        "    dataset = YOLOdataset(images, labels) # make pytorch dataset\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=dataset.custom_collate_fn) # make dataloader (with pytorch dataset)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "iCvB1u-0lduk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_src_path = Path(r\"C:\\Users\\emman\\OneDrive\\Documents\\MI Projects\\Hot Dog Detection YOLO\")\n",
        "coco_src_path = Path(r\"C:\\Users\\emman\\OneDrive\\Documents\\MI Projects\\Hot Dog Detection COCO\\Hot Dog Detection COCO\")"
      ],
      "metadata": {
        "id": "fnAOM-xeLtSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}